{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T21:37:17.170273Z",
     "start_time": "2025-03-03T21:37:16.587742Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from utils import normalize_text, clean_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv(\"data_project.csv\", sep='\\t')\n",
    "\n",
    "print(\"Colonnes disponibles :\", df.columns.tolist())\n",
    "print(\"Aperçu des 5 premières lignes :\")\n",
    "df.head(1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles : ['venue', 'abstract', 'authors', 'n_citation', 'references', 'title', 'year', 'id', 'class']\n",
      "Aperçu des 5 premières lignes :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               venue  \\\n",
       "0  international conference on human-computer int...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The purpose of this study is to develop a lear...   \n",
       "\n",
       "                                             authors  n_citation  \\\n",
       "0  ['Makoto Satoh', 'Ryo Muramatsu', 'Mizue Kayam...           0   \n",
       "\n",
       "                                          references  \\\n",
       "0  ['51c7e02e-f5ed-431a-8cf5-f761f266d4be', '69b6...   \n",
       "\n",
       "                                               title  year  \\\n",
       "0  Preliminary Design of a Network Protocol Learn...  2013   \n",
       "\n",
       "                                     id  class  \n",
       "0  00127ee2-cb05-48ce-bc49-9de556b93346      3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>international conference on human-computer int...</td>\n",
       "      <td>The purpose of this study is to develop a lear...</td>\n",
       "      <td>['Makoto Satoh', 'Ryo Muramatsu', 'Mizue Kayam...</td>\n",
       "      <td>0</td>\n",
       "      <td>['51c7e02e-f5ed-431a-8cf5-f761f266d4be', '69b6...</td>\n",
       "      <td>Preliminary Design of a Network Protocol Learn...</td>\n",
       "      <td>2013</td>\n",
       "      <td>00127ee2-cb05-48ce-bc49-9de556b93346</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:40:48.034239Z",
     "start_time": "2025-03-03T21:40:37.671201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Options disponibles : \"none\" (pas de normalisation), \"stem\" (pour le stemming) ou \"lemma\" (pour la lemmatization)\n",
    "normalization_method = \"lemma\"  # Remplacez par \"stem\" ou \"lemma\" selon votre préférence\n",
    "\n",
    "# 1. Combinaison du titre et de l'abstract\n",
    "# Si l'abstract est manquant, on utilise une chaîne vide\n",
    "df['text_full'] = df['title'].astype(str) + \" \" + df['abstract'].fillna(\"\")\n",
    "\n",
    "# 2. Application du nettoyage avec la méthode de normalisation choisie\n",
    "df['clean_text'] = df['text_full'].apply(lambda x: clean_text(x, normalization_method=normalization_method))\n",
    "\n",
    "# Affichage des premières lignes pour vérification\n",
    "df['clean_text'].head()\n"
   ],
   "id": "99f01814d152f587",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    preliminary design of a network protocol learn...\n",
       "1    a methodology for the physically accurate visu...\n",
       "2    comparison of garch neural network and support...\n",
       "3    comparing gng3d and quadric error metric metho...\n",
       "4    improved secret image sharing method by encodi...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:40:48.928710Z",
     "start_time": "2025-03-03T21:40:48.054167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Création du CountVectorizer pour le schéma TF\n",
    "vectorizer_tf = CountVectorizer(stop_words='english', min_df=1, max_df=0.95)\n",
    "X_tf = vectorizer_tf.fit_transform(df['clean_text'])\n",
    "\n",
    "# Affichage de la forme de la matrice TF\n",
    "print(\"Forme de la matrice TF :\", X_tf.shape)\n"
   ],
   "id": "8b121a76c4949ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice TF : (40596, 55324)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:40:49.803070Z",
     "start_time": "2025-03-03T21:40:48.945246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Création du TfidfVectorizer pour le schéma TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', min_df=1, max_df=0.95)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['clean_text'])\n",
    "\n",
    "# Affichage de la forme de la matrice TF-IDF\n",
    "print(\"Forme de la matrice TF-IDF :\", X_tfidf.shape)\n"
   ],
   "id": "67b4f0c3a378340b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice TF-IDF : (40596, 55324)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:40:49.905583Z",
     "start_time": "2025-03-03T21:40:49.862072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calcul des occurrences totales des mots\n",
    "word_counts = np.array(X_tf.sum(axis=0)).flatten()\n",
    "word_freq = dict(zip(vectorizer_tf.get_feature_names_out(), word_counts))\n",
    "\n",
    "# Affichage des mots les plus fréquents\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Mots les plus fréquents :\")\n",
    "for word, freq in sorted_word_freq[:10]:  # Affiche les 10 mots les plus fréquents\n",
    "    print(f\"{word}: {freq}\")\n"
   ],
   "id": "605ffe37c9a858fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots les plus fréquents :\n",
      "based: 16170\n",
      "model: 13702\n",
      "data: 11684\n",
      "paper: 11620\n",
      "algorithm: 10682\n",
      "method: 10397\n",
      "problem: 9773\n",
      "using: 9771\n",
      "approach: 9472\n",
      "network: 8516\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:45:13.241130Z",
     "start_time": "2025-03-03T21:44:36.758218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction pour obtenir le top 10 des mots et la taille du vocabulaire\n",
    "def get_top_words_and_vocab_size(vectorizer, X):\n",
    "    word_counts = np.array(X.sum(axis=0)).flatten()\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts))\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_words = sorted_word_freq[:10]\n",
    "    vocab_size = len(word_freq)\n",
    "    return top_10_words, vocab_size\n",
    "\n",
    "# Comparaison des méthodes de normalisation\n",
    "normalization_methods = [\"none\", \"stem\", \"lemma\"]\n",
    "results = {}\n",
    "\n",
    "for method in normalization_methods:\n",
    "    # Nettoyage et normalisation du texte\n",
    "    df['clean_text'] = df['text_full'].apply(lambda x: clean_text(x, normalization_method=method))\n",
    "\n",
    "    # Création du CountVectorizer pour le schéma TF\n",
    "    vectorizer_tf = CountVectorizer(stop_words='english', min_df=1, max_df=0.95)\n",
    "    X_tf = vectorizer_tf.fit_transform(df['clean_text'])\n",
    "\n",
    "    # Obtention du top 10 des mots et de la taille du vocabulaire\n",
    "    top_10_words, vocab_size = get_top_words_and_vocab_size(vectorizer_tf, X_tf)\n",
    "\n",
    "    # Stockage des résultats\n",
    "    results[method] = {\n",
    "        \"top_10_words\": top_10_words,\n",
    "        \"vocab_size\": vocab_size\n",
    "    }\n",
    "\n",
    "# Affichage des résultats\n",
    "for method, result in results.items():\n",
    "    print(f\"\\nMéthode de normalisation : {method}\")\n",
    "    print(\"Top 10 des mots :\")\n",
    "    for word, freq in result[\"top_10_words\"]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "    print(f\"Taille du vocabulaire : {result['vocab_size']}\")\n"
   ],
   "id": "d2c26b685af8ff48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Méthode de normalisation : none\n",
      "Top 10 des mots :\n",
      "based: 16170\n",
      "data: 11684\n",
      "paper: 11457\n",
      "using: 9771\n",
      "model: 9636\n",
      "approach: 7588\n",
      "information: 7554\n",
      "systems: 7431\n",
      "algorithm: 7105\n",
      "method: 6907\n",
      "Taille du vocabulaire : 59716\n",
      "\n",
      "Méthode de normalisation : stem\n",
      "Top 10 des mots :\n",
      "thi: 24270\n",
      "use: 21694\n",
      "base: 17269\n",
      "model: 16690\n",
      "data: 11684\n",
      "paper: 11620\n",
      "algorithm: 10909\n",
      "method: 10447\n",
      "problem: 9799\n",
      "propos: 9607\n",
      "Taille du vocabulaire : 43293\n",
      "\n",
      "Méthode de normalisation : lemma\n",
      "Top 10 des mots :\n",
      "based: 16170\n",
      "model: 13702\n",
      "data: 11684\n",
      "paper: 11620\n",
      "algorithm: 10682\n",
      "method: 10397\n",
      "problem: 9773\n",
      "using: 9771\n",
      "approach: 9472\n",
      "network: 8516\n",
      "Taille du vocabulaire : 55324\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse des résultats de la représentation textuelle\n",
    "\n",
    "## 1. Taille du vocabulaire\n",
    "\n",
    "- **Taille du vocabulaire (TF et TF-IDF)** :\n",
    "  Les deux approches ont généré un vocabulaire de **23360 termes**.\n",
    "  Cela indique que les paramètres de filtrage (min_df=2 et max_df=0.95) et le prétraitement (nettoyage, lemmatisation) ont été appliqués de manière cohérente pour les deux méthodes.\n",
    "\n",
    "## 2. Fréquence des termes (TF)\n",
    "\n",
    "- **Top 10 des mots les plus fréquents (TF)** :\n",
    "  - based: 16170\n",
    "  - model: 13702\n",
    "  - data: 11684\n",
    "  - paper: 11620\n",
    "  - algorithm: 10682\n",
    "  - method: 10397\n",
    "  - problem: 9773\n",
    "  - using: 9771\n",
    "  - approach: 9472\n",
    "  - network: 8516\n",
    "\n",
    "- **Interprétation** :\n",
    "  Ces termes reflètent typiquement un corpus scientifique en informatique, où les mots liés aux méthodes, aux modèles et aux données sont naturellement fréquents. La présence de termes comme *algorithm*, *model* ou *network* confirme la cohérence avec le domaine traité.\n",
    "\n",
    "## 3. Informations sur les matrices TF et TF-IDF\n",
    "\n",
    "- **Dimensions** :\n",
    "  Les deux matrices ont la même taille : **40596 documents x 23360 termes**.\n",
    "\n",
    "- **Densité** :\n",
    "  La densité de 0.0012 montre que la matrice est très creuse, ce qui est attendu pour un grand corpus textuel où la majorité des termes ne sont présents que dans quelques documents.\n",
    "\n",
    "- **Comparaison** :\n",
    "  Bien que la structure (dimensions et densité) soit identique pour la matrice TF et celle TF-IDF, les valeurs numériques diffèrent.\n",
    "  La pondération TF-IDF réduit l'importance des termes trop fréquents (moins discriminants) tout en mettant en avant les termes qui, bien que moins fréquents, apportent une information plus spécifique sur le contenu des documents.\n",
    "\n",
    "## 4. Exemple de valeurs pour le document 0\n",
    "\n",
    "- **Observations** :\n",
    "  Pour le document 0, quelques termes et leurs valeurs associées sont :\n",
    "\n",
    "  | Terme    | TF   | TF-IDF  |\n",
    "  |----------|------|---------|\n",
    "  | tool     | 4.0  | 0.3367  |\n",
    "  | network  | 3.0  | 0.2067  |\n",
    "  | learning | 3.0  | 0.2322  |\n",
    "  | student  | 3.0  | 0.3420  |\n",
    "  | design   | 2.0  | 0.1533  |\n",
    "  | protocol | 2.0  | 0.2026  |\n",
    "  | high     | 2.0  | 0.1648  |\n",
    "  | school   | 2.0  | 0.2838  |\n",
    "  | study    | 2.0  | 0.1422  |\n",
    "  | develop  | 2.0  | 0.2034  |\n",
    "\n",
    "- **Analyse** :\n",
    "  - **TF (Term Frequency)** : Indique le nombre d'occurrences du terme dans le document.\n",
    "  - **TF-IDF** : Intègre la fréquence du terme dans le document et son inverse de fréquence dans l'ensemble du corpus.\n",
    "    - Par exemple, le terme *student* apparaît 3 fois avec un TF-IDF de 0.3420, ce qui suggère qu'il est relativement discriminant dans ce document comparé à d'autres termes plus courants dans le corpus.\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "- Le prétraitement (nettoyage, lemmatisation) a permis d’uniformiser les termes, réduisant ainsi la variance du vocabulaire tout en conservant des informations pertinentes pour l'analyse.\n",
    "- L'analyse des fréquences confirme que le corpus est en adéquation avec un domaine scientifique, avec des termes spécifiques au domaine de l'informatique.\n",
    "- La comparaison entre les représentations TF et TF-IDF offre deux perspectives :\n",
    "  - **TF** donne une mesure brute des occurrences,\n",
    "  - **TF-IDF** valorise les termes discriminants et atténue l'effet des termes très fréquents mais moins informatifs.\n"
   ],
   "id": "784148f0056a7df5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparaison des Méthodes de Normalisation\n",
    "\n",
    "| **Méthode de Normalisation** | **Top 10 des mots**                                                                 | **Taille du vocabulaire** |\n",
    "|------------------------------|------------------------------------------------------------------------------------|---------------------------|\n",
    "| Aucune                       | 1. based: 16170<br>2. data: 11684<br>3. paper: 11457<br>4. using: 9771<br>5. model: 9636<br>6. approach: 7588<br>7. information: 7554<br>8. systems: 7431<br>9. algorithm: 7105<br>10. method: 6907 | 59716                     |\n",
    "| Stemming                     | 1. thi: 24270<br>2. use: 21694<br>3. base: 17269<br>4. model: 16690<br>5. data: 11684<br>6. paper: 11620<br>7. algorithm: 10909<br>8. method: 10447<br>9. problem: 9799<br>10. propos: 9607 | 43293                     |\n",
    "| Lemmatisation                | 1. based: 16170<br>2. model: 13702<br>3. data: 11684<br>4. paper: 11620<br>5. algorithm: 10682<br>6. method: 10397<br>7. problem: 9773<br>8. using: 9771<br>9. approach: 9472<br>10. network: 8516 | 55324                     |\n",
    "\n",
    "## Exercice 3\n"
   ],
   "id": "df1451ac9b4463b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:58:40.958070Z",
     "start_time": "2025-03-03T21:58:40.939745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction pour traiter une requête sous forme de chaîne et la transformer en vecteur TF-IDF\n",
    "def process_query(query_string, vectorizer):\n",
    "    # Transformation de la requête en vecteur TF-IDF\n",
    "    query_vector = vectorizer.transform([query_string])\n",
    "    return query_vector\n",
    "\n",
    "# Exemple de requête sous forme de chaîne\n",
    "query_string = \"Is the Macintosh Dead Meat\"\n",
    "\n",
    "# Transformation de la requête en vecteur TF-IDF\n",
    "query_vector = process_query(query_string, vectorizer_tfidf)\n",
    "\n",
    "# Calcul de la similarité cosinus entre le vecteur-requête et les vecteurs documents\n",
    "cosine_similarities = cosine_similarity(query_vector, X_tfidf).flatten()\n",
    "\n",
    "# Filtrer les documents ayant un score de similarité supérieur à 0\n",
    "filtered_indices = [i for i in range(len(cosine_similarities)) if cosine_similarities[i] > 0]\n",
    "\n",
    "# Trier les indices filtrés par ordre de similarité décroissante\n",
    "sorted_filtered_indices = sorted(filtered_indices, key=lambda i: cosine_similarities[i], reverse=True)\n",
    "\n",
    "# Récupération des documents les plus pertinents\n",
    "top_n = 5  # Nombre de documents à afficher\n",
    "top_documents = df.iloc[sorted_filtered_indices[:top_n]].copy()\n",
    "\n",
    "# Ajout des scores de similarité au DataFrame\n",
    "top_documents['similarity_score'] = [cosine_similarities[i] for i in sorted_filtered_indices[:top_n]]\n",
    "\n",
    "# Affichage des documents pertinents dans un tableau DataFrame avec les scores de similarité\n",
    "if not top_documents.empty:\n",
    "    print(\"\\nDocuments les plus pertinents :\")\n",
    "    display(top_documents[['title', 'abstract', 'similarity_score']])\n",
    "else:\n",
    "    print(\"\\nAucun document pertinent trouvé.\")\n"
   ],
   "id": "d3643efb7c59cc29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents les plus pertinents :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                   title abstract  \\\n",
       "30215                         Is the Macintosh Dead Meat      NaN   \n",
       "23117  Dead Block Placement Avoidance in L1 Data Caches.      NaN   \n",
       "40066  Dead Reckoning for Mobile Robots Using Two Opt...      NaN   \n",
       "40073  ESTIMATION AND COMPENSATION OF DEAD-ZONE INHER...      NaN   \n",
       "24061  DOUBLE PULSE TRANSMISSION - DEAD ZONE DECREASI...      NaN   \n",
       "\n",
       "       similarity_score  \n",
       "30215          1.000000  \n",
       "23117          0.224900  \n",
       "40066          0.222685  \n",
       "40073          0.215279  \n",
       "24061          0.194699  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30215</th>\n",
       "      <td>Is the Macintosh Dead Meat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23117</th>\n",
       "      <td>Dead Block Placement Avoidance in L1 Data Caches.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40066</th>\n",
       "      <td>Dead Reckoning for Mobile Robots Using Two Opt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40073</th>\n",
       "      <td>ESTIMATION AND COMPENSATION OF DEAD-ZONE INHER...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.215279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24061</th>\n",
       "      <td>DOUBLE PULSE TRANSMISSION - DEAD ZONE DECREASI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.194699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pour compérer tf et tf x idf\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction pour traiter une requête sous forme de chaîne et la transformer en vecteur\n",
    "def process_query(query_string, vectorizer):\n",
    "    # Transformation de la requête en vecteur\n",
    "    query_vector = vectorizer.transform([query_string])\n",
    "    return query_vector\n",
    "\n",
    "# Exemple de requête sous forme de chaîne\n",
    "query_string = \"machine learning model\"\n",
    "\n",
    "# Utilisation de CountVectorizer pour le schéma TF\n",
    "vectorizer_tf = CountVectorizer(stop_words='english', min_df=1, max_df=0.95)\n",
    "X_tf = vectorizer_tf.fit_transform(df['clean_text'])\n",
    "\n",
    "# Utilisation de TfidfVectorizer pour le schéma TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', min_df=1, max_df=0.95)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['clean_text'])\n",
    "\n",
    "# Transformation de la requête en vecteurs TF et TF-IDF\n",
    "query_vector_tf = process_query(query_string, vectorizer_tf)\n",
    "query_vector_tfidf = process_query(query_string, vectorizer_tfidf)\n",
    "\n",
    "# Calcul de la similarité cosinus pour TF et TF-IDF\n",
    "cosine_similarities_tf = cosine_similarity(query_vector_tf, X_tf).flatten()\n",
    "cosine_similarities_tfidf = cosine_similarity(query_vector_tfidf, X_tfidf).flatten()\n",
    "\n",
    "# Filtrer et trier les documents pour TF\n",
    "filtered_indices_tf = [i for i in range(len(cosine_similarities_tf)) if cosine_similarities_tf[i] > 0]\n",
    "sorted_filtered_indices_tf = sorted(filtered_indices_tf, key=lambda i: cosine_similarities_tf[i], reverse=True)\n",
    "\n",
    "# Filtrer et trier les documents pour TF-IDF\n",
    "filtered_indices_tfidf = [i for i in range(len(cosine_similarities_tfidf)) if cosine_similarities_tfidf[i] > 0]\n",
    "sorted_filtered_indices_tfidf = sorted(filtered_indices_tfidf, key=lambda i: cosine_similarities_tfidf[i], reverse=True)\n",
    "\n",
    "# Récupération des documents les plus pertinents\n",
    "top_n = 5  # Nombre de documents à afficher\n",
    "top_documents_tf = df.iloc[sorted_filtered_indices_tf[:top_n]].copy()\n",
    "top_documents_tfidf = df.iloc[sorted_filtered_indices_tfidf[:top_n]].copy()\n",
    "\n",
    "# Ajout des scores de similarité aux DataFrames\n",
    "top_documents_tf['similarity_score'] = [cosine_similarities_tf[i] for i in sorted_filtered_indices_tf[:top_n]]\n",
    "top_documents_tfidf['similarity_score'] = [cosine_similarities_tfidf[i] for i in sorted_filtered_indices_tfidf[:top_n]]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nComparaison des schémas de pondération pour la requête :\", query_string)\n",
    "\n",
    "print(\"\\nDocuments les plus pertinents (schéma TF) :\")\n",
    "if not top_documents_tf.empty:\n",
    "    display(top_documents_tf[['title', 'abstract', 'similarity_score']])\n",
    "else:\n",
    "    print(\"Aucun document pertinent trouvé.\")\n",
    "\n",
    "print(\"\\nDocuments les plus pertinents (schéma TF-IDF) :\")\n",
    "if not top_documents_tfidf.empty:\n",
    "    display(top_documents_tfidf[['title', 'abstract', 'similarity_score']])\n",
    "else:\n",
    "    print(\"Aucun document pertinent trouvé.\")\n"
   ],
   "id": "6281d31bfa0b54ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T22:05:12.912015Z",
     "start_time": "2025-03-03T22:05:11.063415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def search_documents(query, df, normalization_method=\"none\",\n",
    "                     weighting_scheme=\"tf\", vocab_size=None,\n",
    "                     include_stopwords=False, similarity_measure=\"cosine\",\n",
    "                     top_n=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Réalise une recherche sur le corpus à partir d'une requête textuelle,\n",
    "    avec différentes options de configuration.\n",
    "\n",
    "    Paramètres :\n",
    "      - query : chaîne de caractères saisie par l'utilisateur.\n",
    "      - df : le DataFrame contenant les documents avec une colonne 'clean_text'.\n",
    "      - normalization_method : méthode de normalisation à utiliser (\"none\", \"stem\" ou \"lemma\").\n",
    "      - weighting_scheme : schéma de pondération (\"tf\" ou \"tfidf\").\n",
    "      - vocab_size : taille du vocabulaire à utiliser (None = tout le vocabulaire).\n",
    "      - include_stopwords : inclure ou non les mots-outils.\n",
    "      - similarity_measure : mesure de similarité à utiliser (\"cosine\" ou \"euclidean\").\n",
    "      - top_n : nombre de résultats à afficher.\n",
    "      - verbose : si True, affiche les détails de configuration et les résultats.\n",
    "\n",
    "    Retourne :\n",
    "      - Une liste des indices des documents les plus similaires\n",
    "      - Une liste des scores de similarité correspondants\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n=== CONFIGURATION DU MOTEUR DE RECHERCHE ===\")\n",
    "        print(f\"- Méthode de normalisation: {normalization_method}\")\n",
    "        print(f\"- Schéma de pondération: {weighting_scheme}\")\n",
    "        print(f\"- Taille du vocabulaire: {'complète' if vocab_size is None else vocab_size}\")\n",
    "        print(f\"- Mots-outils: {'inclus' if include_stopwords else 'exclus'}\")\n",
    "        print(f\"- Mesure de similarité: {similarity_measure}\")\n",
    "\n",
    "    # Appliquer le même prétraitement sur la requête\n",
    "    query_clean = clean_text(query, normalization_method=normalization_method)\n",
    "\n",
    "    # Configuration du vectorizer\n",
    "    stop_words = None if include_stopwords else 'english'\n",
    "\n",
    "    # Choix du type de vectorizer en fonction du schéma de pondération\n",
    "    if weighting_scheme == \"tf\":\n",
    "        vectorizer = CountVectorizer(stop_words=stop_words, min_df=2, max_df=0.95)\n",
    "    else:  # \"tfidf\"\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2, max_df=0.95)\n",
    "\n",
    "    # Entraînement du vectorizer sur les documents\n",
    "    X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "    # Si une taille de vocabulaire est spécifiée, on limite le vocabulaire\n",
    "    if vocab_size is not None and vocab_size < len(vectorizer.vocabulary_):\n",
    "        # Calcul des occurrences totales de chaque mot dans le corpus\n",
    "        word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "        vocab_counts = {word: word_counts[idx] for word, idx in vectorizer.vocabulary_.items()}\n",
    "\n",
    "        # Sélection des N mots les plus fréquents\n",
    "        sorted_vocab = sorted(vocab_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        limited_vocab = [word for word, _ in sorted_vocab[:vocab_size]]\n",
    "\n",
    "        # Recréation du vectorizer avec le vocabulaire limité\n",
    "        if weighting_scheme == \"tf\":\n",
    "            vectorizer = CountVectorizer(vocabulary=limited_vocab)\n",
    "        else:  # \"tfidf\"\n",
    "            vectorizer = TfidfVectorizer(vocabulary=limited_vocab)\n",
    "\n",
    "        # Réentraînement avec le vocabulaire limité\n",
    "        X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"- Vocabulaire limité aux {vocab_size} mots les plus fréquents\")\n",
    "\n",
    "    # Transformation de la requête en vecteur\n",
    "    query_vector = vectorizer.transform([query_clean])\n",
    "\n",
    "    # Calcul de la similarité selon la mesure choisie\n",
    "    if similarity_measure == \"cosine\":\n",
    "        similarities = cosine_similarity(query_vector, X).flatten()\n",
    "    else:  # \"euclidean\"\n",
    "        # Pour la distance euclidienne, on inverse les valeurs car une distance plus petite\n",
    "        # signifie une plus grande similarité\n",
    "        distances = euclidean_distances(query_vector, X).flatten()\n",
    "        similarities = 1 / (1 + distances)  # Transformation en similarité\n",
    "\n",
    "    # Trier les indices des documents par ordre décroissant de similarité\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "    sorted_scores = similarities[sorted_indices]\n",
    "\n",
    "    # Affichage des résultats si verbose est activé\n",
    "    if verbose:\n",
    "        print(f\"\\nTop {top_n} documents pour la requête : '{query}'\\n\")\n",
    "        for idx, score in zip(sorted_indices[:top_n], sorted_scores[:top_n]):\n",
    "            title = df.loc[idx, 'title']\n",
    "            abstract = df.loc[idx, 'abstract'] if pd.notnull(df.loc[idx, 'abstract']) else \"Abstract manquant\"\n",
    "            print(f\"Score: {score:.4f}\")\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Abstract: {abstract}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    return sorted_indices[:top_n], sorted_scores[:top_n]\n",
    "\n",
    "# Fonction pour comparer différentes configurations du moteur de recherche\n",
    "def compare_search_configurations(query, df, configs, top_n=5):\n",
    "    \"\"\"\n",
    "    Compare les résultats de recherche avec différentes configurations.\n",
    "\n",
    "    Paramètres:\n",
    "      - query: la requête à exécuter\n",
    "      - df: le DataFrame contenant les documents\n",
    "      - configs: liste de dictionnaires de paramètres pour search_documents\n",
    "      - top_n: nombre de résultats à afficher pour chaque configuration\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    print(f\"Comparaison des résultats pour la requête: '{query}'\")\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\nConfiguration #{i+1}:\")\n",
    "        for param, value in config.items():\n",
    "            if param != 'df':  # Ne pas afficher le DataFrame\n",
    "                print(f\"- {param}: {value}\")\n",
    "\n",
    "        # Exécuter la recherche avec cette configuration\n",
    "        indices, scores = search_documents(query, df, **{**config, 'verbose': False, 'top_n': top_n})\n",
    "\n",
    "        # Afficher les résultats\n",
    "        print(f\"\\nTop {min(top_n, len(indices))} résultats:\")\n",
    "        for j, (idx, score) in enumerate(zip(indices, scores)):\n",
    "            print(f\"{j+1}. Score: {score:.4f}, Titre: {df.loc[idx, 'title']}\")\n",
    "\n",
    "        all_results.append({\n",
    "            'config': config,\n",
    "            'indices': indices,\n",
    "            'scores': scores\n",
    "        })\n",
    "\n",
    "    # Analyse de la diversité des résultats\n",
    "    if len(configs) > 1:\n",
    "        print(\"\\nAnalyse de la diversité des résultats:\")\n",
    "        for i in range(len(configs)):\n",
    "            for j in range(i+1, len(configs)):\n",
    "                # Calcul du taux de recouvrement entre les résultats des configurations i et j\n",
    "                set_i = set(all_results[i]['indices'])\n",
    "                set_j = set(all_results[j]['indices'])\n",
    "                overlap = len(set_i.intersection(set_j))\n",
    "                overlap_rate = overlap / min(len(set_i), len(set_j))\n",
    "\n",
    "                print(f\"Recouvrement entre config #{i+1} et config #{j+1}: {overlap}/{min(len(set_i), len(set_j))} documents ({overlap_rate:.2%})\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Exemple d'utilisation pour comparer différentes configurations\n",
    "def run_search_comparison(query, df):\n",
    "    \"\"\"\n",
    "    Exemple d'utilisation montrant comment comparer différentes configurations\n",
    "    du moteur de recherche pour une requête donnée.\n",
    "    \"\"\"\n",
    "    # Définir différentes configurations à tester\n",
    "    configs = [\n",
    "        {\n",
    "            # Configuration de base: TF, sans mots-outils, similarité cosinus\n",
    "            'normalization_method': 'none',\n",
    "            'weighting_scheme': 'tf',\n",
    "            'include_stopwords': False,\n",
    "            'similarity_measure': 'cosine',\n",
    "            'vocab_size': None\n",
    "        },\n",
    "        {\n",
    "            # Configuration avec TF-IDF\n",
    "            'normalization_method': 'none',\n",
    "            'weighting_scheme': 'tfidf',\n",
    "            'include_stopwords': False,\n",
    "            'similarity_measure': 'cosine',\n",
    "            'vocab_size': None\n",
    "        },\n",
    "        {\n",
    "            # Configuration avec vocabulaire limité\n",
    "            'normalization_method': 'none',\n",
    "            'weighting_scheme': 'tf',\n",
    "            'include_stopwords': False,\n",
    "            'similarity_measure': 'cosine',\n",
    "            'vocab_size': 500\n",
    "        },\n",
    "        {\n",
    "            # Configuration avec mots-outils\n",
    "            'normalization_method': 'none',\n",
    "            'weighting_scheme': 'tf',\n",
    "            'include_stopwords': True,\n",
    "            'similarity_measure': 'cosine',\n",
    "            'vocab_size': None\n",
    "        },\n",
    "        {\n",
    "            # Configuration avec distance euclidienne\n",
    "            'normalization_method': 'none',\n",
    "            'weighting_scheme': 'tf',\n",
    "            'include_stopwords': False,\n",
    "            'similarity_measure': 'euclidean',\n",
    "            'vocab_size': None\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Comparer les résultats avec ces différentes configurations\n",
    "    compare_search_configurations(query, df, configs)\n",
    "\n",
    "# Exemple d'utilisation avec une requête spécifique\n",
    "# run_search_comparison(\"machine learning\", df)"
   ],
   "id": "46a9a0c401768985",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparaison des schémas de pondération pour la requête : machine learning model\n",
      "\n",
      "Documents les plus pertinents (schéma TF) :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                   title  \\\n",
       "35626  Using Machine Learning Methods to Improve Qual...   \n",
       "36360  Extending extreme learning machine with combin...   \n",
       "13535                        Machine learning and agents   \n",
       "25141  Learning Hebrew Roots: Machine Learning with L...   \n",
       "26165           Machine Learning and Relevance Feedback.   \n",
       "\n",
       "                                                abstract  similarity_score  \n",
       "35626                                                NaN          0.666667  \n",
       "36360  We consider the Extreme Learning Machine model...          0.604145  \n",
       "13535  The paper reviews current research results int...          0.601793  \n",
       "25141                                                NaN          0.577350  \n",
       "26165                                                NaN          0.577350  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35626</th>\n",
       "      <td>Using Machine Learning Methods to Improve Qual...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36360</th>\n",
       "      <td>Extending extreme learning machine with combin...</td>\n",
       "      <td>We consider the Extreme Learning Machine model...</td>\n",
       "      <td>0.604145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13535</th>\n",
       "      <td>Machine learning and agents</td>\n",
       "      <td>The paper reviews current research results int...</td>\n",
       "      <td>0.601793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25141</th>\n",
       "      <td>Learning Hebrew Roots: Machine Learning with L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26165</th>\n",
       "      <td>Machine Learning and Relevance Feedback.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents les plus pertinents (schéma TF-IDF) :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                   title  \\\n",
       "13535                        Machine learning and agents   \n",
       "38803  ASV monitor: creating comparability of machine...   \n",
       "35626  Using Machine Learning Methods to Improve Qual...   \n",
       "33050         Sparse Representation for Machine Learning   \n",
       "32272           Visual Data Mining and Machine Learning.   \n",
       "\n",
       "                                                abstract  similarity_score  \n",
       "13535  The paper reviews current research results int...          0.603422  \n",
       "38803  In this demonstration paper we present an appl...          0.568691  \n",
       "35626                                                NaN          0.566570  \n",
       "33050                                                NaN          0.552298  \n",
       "32272                                                NaN          0.537334  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13535</th>\n",
       "      <td>Machine learning and agents</td>\n",
       "      <td>The paper reviews current research results int...</td>\n",
       "      <td>0.603422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38803</th>\n",
       "      <td>ASV monitor: creating comparability of machine...</td>\n",
       "      <td>In this demonstration paper we present an appl...</td>\n",
       "      <td>0.568691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35626</th>\n",
       "      <td>Using Machine Learning Methods to Improve Qual...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33050</th>\n",
       "      <td>Sparse Representation for Machine Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.552298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32272</th>\n",
       "      <td>Visual Data Mining and Machine Learning.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.537334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T21:47:45.928842Z",
     "start_time": "2025-03-03T21:47:40.613674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"beach aristocrat\"  # Exemple de requête\n",
    "run_search_comparison(query, df)"
   ],
   "id": "7b46f0f0f4e5a5d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison des résultats pour la requête: 'beach aristocrat'\n",
      "\n",
      "Configuration #1:\n",
      "- normalization_method: none\n",
      "- weighting_scheme: tf\n",
      "- include_stopwords: False\n",
      "- similarity_measure: cosine\n",
      "- vocab_size: None\n",
      "\n",
      "Top 5 résultats:\n",
      "1. Score: 0.0729, Titre: Development of an Information Portal for the Lake Winnipeg Basin Initiative\n",
      "2. Score: 0.0702, Titre: An adaptive pitch control strategy for a doubly fed wind generation system\n",
      "3. Score: 0.0000, Titre: An object-oriented denotational semantics of a small programming language.\n",
      "4. Score: 0.0000, Titre: Extending the Object-Process Methodology to Handle Real-Time Systems.\n",
      "5. Score: 0.0000, Titre: Query Execution for RDF Data on Row and Column Store\n",
      "\n",
      "Configuration #2:\n",
      "- normalization_method: none\n",
      "- weighting_scheme: tfidf\n",
      "- include_stopwords: False\n",
      "- similarity_measure: cosine\n",
      "- vocab_size: None\n",
      "\n",
      "Top 5 résultats:\n",
      "1. Score: 0.1133, Titre: Development of an Information Portal for the Lake Winnipeg Basin Initiative\n",
      "2. Score: 0.1112, Titre: An adaptive pitch control strategy for a doubly fed wind generation system\n",
      "3. Score: 0.0000, Titre: An object-oriented denotational semantics of a small programming language.\n",
      "4. Score: 0.0000, Titre: Extending the Object-Process Methodology to Handle Real-Time Systems.\n",
      "5. Score: 0.0000, Titre: Query Execution for RDF Data on Row and Column Store\n",
      "\n",
      "Configuration #3:\n",
      "- normalization_method: none\n",
      "- weighting_scheme: tf\n",
      "- include_stopwords: False\n",
      "- similarity_measure: cosine\n",
      "- vocab_size: 500\n",
      "\n",
      "Top 5 résultats:\n",
      "1. Score: 0.0000, Titre: An object-oriented denotational semantics of a small programming language.\n",
      "2. Score: 0.0000, Titre: Non-newtonian blood flow analysis for the portal vein based on a CT image\n",
      "3. Score: 0.0000, Titre: Machine learning and agents\n",
      "4. Score: 0.0000, Titre: Query Execution for RDF Data on Row and Column Store\n",
      "5. Score: 0.0000, Titre: Space Hierarchy Results for Randomized and Other Semantic Models.\n",
      "\n",
      "Configuration #4:\n",
      "- normalization_method: none\n",
      "- weighting_scheme: tf\n",
      "- include_stopwords: True\n",
      "- similarity_measure: cosine\n",
      "- vocab_size: None\n",
      "\n",
      "Top 5 résultats:\n",
      "1. Score: 0.0474, Titre: An adaptive pitch control strategy for a doubly fed wind generation system\n",
      "2. Score: 0.0439, Titre: Development of an Information Portal for the Lake Winnipeg Basin Initiative\n",
      "3. Score: 0.0000, Titre: An object-oriented denotational semantics of a small programming language.\n",
      "4. Score: 0.0000, Titre: Extending the Object-Process Methodology to Handle Real-Time Systems.\n",
      "5. Score: 0.0000, Titre: Query Execution for RDF Data on Row and Column Store\n",
      "\n",
      "Configuration #5:\n",
      "- normalization_method: none\n",
      "- weighting_scheme: tf\n",
      "- include_stopwords: False\n",
      "- similarity_measure: euclidean\n",
      "- vocab_size: None\n",
      "\n",
      "Top 5 résultats:\n",
      "1. Score: 0.5000, Titre: Erkennbarkeit logischer Schaltzeichen.\n",
      "2. Score: 0.5000, Titre: Is What You See What You Get\n",
      "3. Score: 0.5000, Titre: Neurocontrol: H. Tolle and E. Ersu ☆\n",
      "4. Score: 0.5000, Titre: System F i .\n",
      "5. Score: 0.5000, Titre: All-to-All\n",
      "\n",
      "Analyse de la diversité des résultats:\n",
      "Recouvrement entre config #1 et config #2: 5/5 documents (100.00%)\n",
      "Recouvrement entre config #1 et config #3: 2/5 documents (40.00%)\n",
      "Recouvrement entre config #1 et config #4: 5/5 documents (100.00%)\n",
      "Recouvrement entre config #1 et config #5: 0/5 documents (0.00%)\n",
      "Recouvrement entre config #2 et config #3: 2/5 documents (40.00%)\n",
      "Recouvrement entre config #2 et config #4: 5/5 documents (100.00%)\n",
      "Recouvrement entre config #2 et config #5: 0/5 documents (0.00%)\n",
      "Recouvrement entre config #3 et config #4: 2/5 documents (40.00%)\n",
      "Recouvrement entre config #3 et config #5: 0/5 documents (0.00%)\n",
      "Recouvrement entre config #4 et config #5: 0/5 documents (0.00%)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse des résultats de la comparaison des configurations du moteur de recherche\n",
    "\n",
    "La comparaison a été effectuée pour la requête **\"machine learning\"** en testant cinq configurations différentes. Voici un résumé et une analyse détaillée :\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Configurations testées\n",
    "\n",
    "- **Configuration #1 :**\n",
    "  - **Paramètres :** TF, sans normalisation, sans inclusion de stop words, vocabulaire complet, similarité cosinus.\n",
    "  - **Résultats :**\n",
    "    Les documents affichés (scores autour de 0.7071 pour les quatre premiers et 0.6937 pour le cinquième) suggèrent une forte similarité entre la requête et les documents sélectionnés, avec une cohérence dans le classement.\n",
    "\n",
    "- **Configuration #2 :**\n",
    "  - **Paramètres :** TF-IDF, sans normalisation, sans inclusion de stop words, vocabulaire complet, similarité cosinus.\n",
    "  - **Résultats :**\n",
    "    Les scores sont légèrement plus faibles (allant de 0.6518 à 0.5667) et l'ordre des résultats change par rapport à la configuration #1. Cela reflète l'effet de la pondération TF-IDF qui tend à atténuer l'influence des termes très fréquents et moins discriminants.\n",
    "\n",
    "- **Configuration #3 :**\n",
    "  - **Paramètres :** TF, sans normalisation, sans inclusion de stop words, vocabulaire limité aux 500 mots les plus fréquents, similarité cosinus.\n",
    "  - **Résultats :**\n",
    "    On observe des scores très élevés (deux documents avec un score parfait de 1.0000) et une plus grande différenciation entre les documents. La limitation du vocabulaire semble concentrer la représentation sur un ensemble de termes très pertinents, ce qui conduit à une correspondance plus forte pour certains documents.\n",
    "\n",
    "- **Configuration #4 :**\n",
    "  - **Paramètres :** TF, sans normalisation, avec inclusion de stop words, vocabulaire complet, similarité cosinus.\n",
    "  - **Résultats :**\n",
    "    Les scores sont modérément faibles (entre 0.6708 et 0.5774). L'inclusion des stop words modifie légèrement le profil de la similarité et peut introduire du bruit, entraînant ainsi un classement différent de celui obtenu sans stop words.\n",
    "\n",
    "- **Configuration #5 :**\n",
    "  - **Paramètres :** TF, sans normalisation, sans inclusion de stop words, vocabulaire complet, **mesure de similarité euclidienne**.\n",
    "  - **Résultats :**\n",
    "    Tous les documents affichés ont le même score (0.4142) et les titres semblent moins pertinents par rapport à la requête. Ce résultat illustre que, pour ce corpus et cette requête, la mesure euclidienne ne capture pas efficacement la notion de similarité que la cosinus peut exprimer.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Analyse des recouvrements entre configurations\n",
    "\n",
    "Le taux de recouvrement entre les résultats de différentes configurations indique la similitude des classements obtenus :\n",
    "\n",
    "- **Entre config #1 et #2 :** 60% des documents se recoupent\n",
    "  → Indique que TF et TF-IDF, bien que différents dans la pondération, conduisent à des résultats relativement similaires.\n",
    "\n",
    "- **Entre config #1 et #3 :** 40% de recouvrement\n",
    "  → La limitation du vocabulaire modifie significativement les résultats, concentrant la recherche sur un sous-ensemble de termes.\n",
    "\n",
    "- **Entre config #1 et #4 :** 80% de recouvrement\n",
    "  → L'inclusion des stop words, dans ce cas, n'altère pas énormément le classement, même si quelques ajustements sont observés.\n",
    "\n",
    "- **Entre config #1 et #5 :** 20% de recouvrement\n",
    "  → La différence de mesure de similarité (euclidienne vs cosinus) a un impact très marqué sur les documents retenus.\n",
    "\n",
    "- **Autres recouvrements faibles (0–20%) avec la config #5**\n",
    "  → Confirment que la mesure euclidienne produit un classement radicalement différent, probablement moins pertinent pour ce type de données textuelles.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Conclusion\n",
    "\n",
    "- **Choix de la pondération :**\n",
    "  - **TF vs TF-IDF :** La pondération TF-IDF réduit l'effet des termes très fréquents, modifiant légèrement l'ordre des documents, mais reste globalement cohérente avec TF (60% de recouvrement).\n",
    "\n",
    "- **Taille du vocabulaire :**\n",
    "  - Limiter le vocabulaire (config #3) peut augmenter les scores pour certains documents (scores parfaits), ce qui peut être bénéfique pour une recherche focalisée, mais cela modifie aussi le classement et réduit la diversité des résultats.\n",
    "\n",
    "- **Inclusion des stop words :**\n",
    "  - Leur inclusion (config #4) affecte moins fortement le classement, même si l'on peut observer quelques différences, probablement dues à l'introduction d'un bruit modéré.\n",
    "\n",
    "- **Mesure de similarité :**\n",
    "  - La similarité cosinus semble mieux adaptée aux représentations textuelles (TF et TF-IDF) qu'une mesure basée sur la distance euclidienne, qui, dans ce cas, génère des scores très homogènes et des résultats moins pertinents.\n",
    "\n",
    "Dans l'ensemble, cette comparaison met en évidence l'importance de choisir soigneusement les paramètres (pondération, vocabulaire, stop words, et mesure de similarité) pour optimiser la performance d'un moteur de recherche sur un corpus donné.\n"
   ],
   "id": "b60dced00ae0c6ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac01e0da393dfce3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
